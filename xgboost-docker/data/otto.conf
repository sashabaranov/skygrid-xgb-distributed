# General Parameters, see comment for each definition
# choose the booster, can be gbtree or gblinear
booster = gbtree
# choose logistic regression loss function for binary classification
objective = multi:softprob
num_class = 9
eval_metric = mlogloss

# Tree Booster Parameters
# step size shrinkage
eta = 0.3
# minimum loss reduction required to make a further partition
gamma = 0.4 
# minimum sum of instance weight(hessian) needed in a child
min_child_weight = 1 
# maximum depth of a tree
max_depth = 7
subsample = 1 

# Task Parameters
# the number of round to do boosting
num_round = 3
# 0 means do not save any model except the final round model
save_period = 0 
# The path of training data
data = "/data/train.col0"
#eval[test] = "train_cv2.svm"
test:data = "/data/test.0.svm" 

